{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 : Machine learning avec Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans ce TP, on veut créer un modèle de classification entraîné sur les données qui ont été pré-traitées dans les TPs précédents. Pour que tout le monde reparte du même point, on télécharge le dataset prepared_trainingset (ce sont des fichiers parquet) situé dans le répertoire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.types.IntegerType\n",
       "import org.apache.spark.sql.SQLImplicits\n",
       "import org.apache.spark.sql.{functions=>F}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel}\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.t..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello World ! from Trainer\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7813b19d\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@352eebd6\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // Des réglages optionnels du job spark. Les réglages par défaut fonctionnent très bien pour ce TP.\n",
    "    // On vous donne un exemple de setting quand même\n",
    "    val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "    ))\n",
    "\n",
    "    // Initialisation du SparkSession qui est le point d'entrée vers Spark SQL (donne accès aux dataframes, aux RDD,\n",
    "    // création de tables temporaires, etc., et donc aux mécanismes de distribution des calculs)\n",
    "    val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP 3 Spark : Machine Learning Avec Spark\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    /*******************************************************************************\n",
    "      *         TP 3 : Machine learning avec Spark\n",
    "      * Introduction\n",
    "      * Chargement du DataFrame.\n",
    "      * Utilisation des données textuelles\n",
    "      *      Stage 1 : récupérer les mots des textes\n",
    "      *      Stage 2 : retirer les stop words\n",
    "      *      Stage 3 : computer la partie TF\n",
    "      *      Stage 4 : computer la partie IDF\n",
    "      * Conversion des variables catégorielles en variables numériques\n",
    "      *      Stage 5 : convertir country2 en quantités numériques\n",
    "      *      Stage 6 : convertir currency2 en quantités numériques\n",
    "      *      Stages 7 et 8: One-Hot encoder ces deux catégories\n",
    "      * Mettre les données sous une forme utilisable par Spark.ML\n",
    "      *      Stage 9 : assembler tous les features en un unique vecteur\n",
    "      *      Stage 10 : créer/instancier le modèle de classification\n",
    "      * Création du Pipeline\n",
    "      * Entraînement, test, et sauvegarde du modèle\n",
    "      *      Split des données en training et test sets\n",
    "      *      Entraînement du modèle\n",
    "      *      Test du modèle\n",
    "      * Réglage des hyper-paramètres (a.k.a. tuning) du modèle\n",
    "      *      Grid search\n",
    "      *      Test du modèle\n",
    "      * Supplément\n",
    "      *\n",
    "      ********************************************************************************/\n",
    "    println(\"\\n\")\n",
    "    println(\"Hello World ! from Trainer\")\n",
    "    println(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- deadline2: string (nullable = true)\n",
      " |-- created_at2: string (nullable = true)\n",
      " |-- launched_at2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@41e39d2b\n",
       "import sqlContext.implicits._\n",
       "path_to_data: String = /mnt/d/09_SPARK/git_Flooorent/cours-spark-telecom/data/prepared_trainingset/\n",
       "myDataFrame: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val sc = new SparkContext(sparkConf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "val path_to_data : String = \"/mnt/d/09_SPARK/git_Flooorent/cours-spark-telecom/data/prepared_trainingset/\"\n",
    "//import org.apache.spark.sql.sqlContext.implicits._\n",
    "//val DataFrame = sqlContext.read.option(\"mergeSchema\", \"true\").parquet(\"/mnt/d/09_SPARK/prepared_trainingset\")\n",
    "//val myDataFrame = spark.read.parquet(\"/mnt/d/09_SPARK/prepared_trainingset/*.parquet\")\n",
    "//val myDataFrameWithNull = spark.read.parquet(path_to_data+\"*.parquet\")\n",
    "//myDataFrameWithNull.printSchema() \n",
    "val myDataFrame = spark.read.parquet(path_to_data+\"*.parquet\")\n",
    "myDataFrame.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation des données textuelles :\n",
    "- Stage 1 : récupérer les mots des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|american options ...|\n",
      "|iheadbones bone c...|\n",
      "|the fridge magazi...|\n",
      "|support new men's...|\n",
      "|can('t) a psychol...|\n",
      "|fragmented fate e...|\n",
      "|transport (suspen...|\n",
      "|the secret life o...|\n",
      "|cc survival decep...|\n",
      "|the best protein ...|\n",
      "|paradise falls pa...|\n",
      "|the chalet woodsh...|\n",
      "|vagabond mobile g...|\n",
      "|southern shakespe...|\n",
      "|leviathan: montau...|\n",
      "|the candle tray h...|\n",
      "|sun skin the miss...|\n",
      "|7sonic debut stud...|\n",
      "|the hades pit: a ...|\n",
      "|the fitness refin...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDataFrame\n",
    "    .select(\"text\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|text                                                                                                                                                                                                                                               |tokens|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|american options for greater productivity -- (paper) -- looking to create a hemp based paper/ cardboard recycling manufacturing facility american-options-for-greater-productivity-paper                                                           |23    |\n",
      "|iheadbones bone conduction bluetooth wireless headset wireless bluetooth version of the iheadbones bone conduction earbud alternative iheadbones-bone-conduction-bluetooth-wireless-head                                                           |22    |\n",
      "|the fridge magazine the fridge is a new student-run magazine of literature and art. the-fridge-magazine                                                                                                                                            |18    |\n",
      "|support new men's fashion magnetic apparel & accessories it s been over 100 years since a new men s accessory has been invented. we did it with style  comfort and a magnetic attachment system. support-new-mens-fashion-magnetic-apparel-and-acce|42    |\n",
      "|can('t) a psychological horror film about survival in a world that s actively working to make you fail. cant-0                                                                                                                                     |21    |\n",
      "|fragmented fate experience a modern 2d jrpg. fight alongside an entire planet against nightmarish  high-tech invaders intent on devouring their gods! fragmented-fate                                                                              |24    |\n",
      "|transport (suspended) help ons met een transport web en app in europe. help us with a transport web and app in europe. transport                                                                                                                   |23    |\n",
      "|the secret life of your mobile phone (canceled) a stage show using cutting-edge interception technology to reveal the hidden communications of a gadget many of us carry everywhere... the-secret-life-of-your-mobile-phone                        |36    |\n",
      "|cc survival deception. diplomacy. destruction. the team survival game... where you don t know who your teammates are. inspired by mafia/werewolf. cc-survival                                                                                      |24    |\n",
      "|the best protein drink all natural  no artificial  certified hormone-free  peptide whey isolate.  better digestion.  more hypoallergenic.  gluten free. the-best-protein-drink                                                                     |24    |\n",
      "|paradise falls paradise falls is an open world video game  being developed for ios and android applications. paradise-falls                                                                                                                        |19    |\n",
      "|the chalet woodshop, oklahoma woodworking you are awesome and you deserve awesome handmade products. we use solid wood to create anything from art  cutting boards  and furniture the-chalet-woodshop-oklahoma-woodworking                         |32    |\n",
      "|vagabond mobile gallery vagabond is a series of independent exhibitions curated by oana damir a migratory project about offering emerging artists opportunities vagabond-mobile-gallery                                                            |25    |\n",
      "|southern shakespeare festival bringing free shakespeare to florida and the big bend. southern-shakespeare-festival                                                                                                                                 |15    |\n",
      "|leviathan: montauk to lombok - painting the population creating portrait installations nicaragua and indonesia  with pitstops at vermont studio center and a museum show in sinaola  mexico. leviathan-montauk-to-lombok-painting-the-populatio    |33    |\n",
      "|the candle tray hand made candle trays unique and charming made with recycled reclaimed and sustainable woods. the-candle-tray                                                                                                                     |20    |\n",
      "|sun skin the mission is to spread enlightenment through the idea of materialism by creating our product with a higher understanding and demeanor sun-skin                                                                                          |25    |\n",
      "|7sonic debut studio album making noise using a combination of woods  strings  ivory (not real ivory  that s cruel)  and skins (not real skins  that s cruel too). 7sonic-debut-studio-album                                                        |32    |\n",
      "|the hades pit: a female led sci-fi film (relaunching 2015) a young woman embarks on a mission to rescue her abducted father from the strange forces that took him from their home the-hades-pit-an-independent-feature-film                        |40    |\n",
      "|the fitness refinery our dream is to create a website dedicated to providing comprehensive training and diet routines from bodybuilding to marathon running! the-fitness-refinery                                                                  |26    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|american options ...|[american, option...|\n",
      "|iheadbones bone c...|[iheadbones, bone...|\n",
      "|the fridge magazi...|[the, fridge, mag...|\n",
      "|support new men's...|[support, new, me...|\n",
      "|can('t) a psychol...|[can, t, a, psych...|\n",
      "|fragmented fate e...|[fragmented, fate...|\n",
      "|transport (suspen...|[transport, suspe...|\n",
      "|the secret life o...|[the, secret, lif...|\n",
      "|cc survival decep...|[cc, survival, de...|\n",
      "|the best protein ...|[the, best, prote...|\n",
      "|paradise falls pa...|[paradise, falls,...|\n",
      "|the chalet woodsh...|[the, chalet, woo...|\n",
      "|vagabond mobile g...|[vagabond, mobile...|\n",
      "|southern shakespe...|[southern, shakes...|\n",
      "|leviathan: montau...|[leviathan, monta...|\n",
      "|the candle tray h...|[the, candle, tra...|\n",
      "|sun skin the miss...|[sun, skin, the, ...|\n",
      "|7sonic debut stud...|[7sonic, debut, s...|\n",
      "|the hades pit: a ...|[the, hades, pit,...|\n",
      "|the fitness refin...|[the, fitness, re...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_df9b0f8a2ea3\n",
       "countTokens: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(ArrayType(StringType,true))))\n",
       "regexTokenized: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "val countTokens = udf { (words: Seq[String]) => words.length }\n",
    "//val regexTokenized = tokenizer.transform(myDataFrame.select(\"text\"))\n",
    "val regexTokenized = tokenizer.transform(myDataFrame)\n",
    "regexTokenized.select(\"text\", \"tokens\")\n",
    "    .withColumn(\"tokens\", countTokens(col(\"tokens\"))).show(false)\n",
    "regexTokenized\n",
    "    .select(\"text\",\"tokens\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- deadline2: string (nullable = true)\n",
      " |-- created_at2: string (nullable = true)\n",
      " |-- launched_at2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 2 : retirer les stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|[american, option...|\n",
      "|[iheadbones, bone...|\n",
      "|[the, fridge, mag...|\n",
      "|[support, new, me...|\n",
      "|[can, t, a, psych...|\n",
      "|[fragmented, fate...|\n",
      "|[transport, suspe...|\n",
      "|[the, secret, lif...|\n",
      "|[cc, survival, de...|\n",
      "|[the, best, prote...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_2f4d0da2d085\n",
       "dfFiltered: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(tokenizer.getOutputCol) //  .setInputCol(\"tokens\")\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "val dfFiltered     =   remover.transform(regexTokenized) //.toDF().show(true)\n",
    "dfFiltered.select(\"tokens\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 3 : computer la partie TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- deadline2: string (nullable = true)\n",
      " |-- created_at2: string (nullable = true)\n",
      " |-- launched_at2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFiltered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            filtered|         rawFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[american, option...|(97856,[33,43,91,...|\n",
      "|[iheadbones, bone...|(97856,[700,874,8...|\n",
      "|[fridge, magazine...|(97856,[0,7,87,30...|\n",
      "|[support, new, me...|(97856,[0,60,86,1...|\n",
      "|[psychological, h...|(97856,[3,10,13,6...|\n",
      "|[fragmented, fate...|(97856,[88,114,15...|\n",
      "|[transport, suspe...|(97856,[2,19,44,6...|\n",
      "|[secret, life, mo...|(97856,[11,19,31,...|\n",
      "|[cc, survival, de...|(97856,[9,118,288...|\n",
      "|[best, protein, d...|(97856,[49,79,218...|\n",
      "|[paradise, falls,...|(97856,[9,10,23,1...|\n",
      "|[chalet, woodshop...|(97856,[7,33,184,...|\n",
      "|[vagabond, mobile...|(97856,[4,14,105,...|\n",
      "|[southern, shakes...|(97856,[49,103,11...|\n",
      "|[leviathan, monta...|(97856,[32,42,110...|\n",
      "|[candle, tray, ha...|(97856,[48,90,150...|\n",
      "|[sun, skin, missi...|(97856,[110,571,8...|\n",
      "|[7sonic, debut, s...|(97856,[1,18,42,4...|\n",
      "|[hades, pit, fema...|(97856,[3,52,53,6...|\n",
      "|[fitness, refiner...|(97856,[33,89,260...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_59c226a0eac4\n",
       "tfModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_59c226a0eac4\n",
       "featurizedData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 15 more fields]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fit a CountVectorizerModel from the corpus\n",
    "val countVectorizer: CountVectorizer = new CountVectorizer()\n",
    "  .setInputCol(remover.getOutputCol)\n",
    "  .setOutputCol(\"rawFeatures\")\n",
    "  .setMinDF(1)\n",
    "val tfModel = countVectorizer.fit(dfFiltered)\n",
    "val featurizedData = tfModel.transform(dfFiltered)\n",
    "featurizedData.select(\"filtered\", \"rawFeatures\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val hashingTF = new HashingTF()\n",
    "//    .setInputCol(\"filtered\")\n",
    "//    .setOutputCol(\"rawFeatures\")\n",
    "//    .setNumFeatures(20000)\n",
    "//dfFiltered.getClass\n",
    "//val featurizedData2 = hashingTF.transform(dfF) //.show(true)\n",
    "//featurizedData2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 4 : computer la partie IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- deadline2: string (nullable = true)\n",
      " |-- created_at2: string (nullable = true)\n",
      " |-- launched_at2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_1f9830d082e8\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_1f9830d082e8\n",
       "rescaledData: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "        .setInputCol(countVectorizer.getOutputCol) // .setInputCol(\"rawFeatures\")\n",
    "        .setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des variables catégorielles en variables numériques :\n",
    "- Stage 5 : convertir **country2** en quantités numériques dans une colonne **country_indexed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|country2|country_indexed|\n",
      "+--------+---------------+\n",
      "|      AU|            3.0|\n",
      "|      IE|            9.0|\n",
      "|      US|            0.0|\n",
      "|      GB|            1.0|\n",
      "|      CA|            2.0|\n",
      "|      NO|            8.0|\n",
      "|      DE|           10.0|\n",
      "|      DK|            7.0|\n",
      "|      NL|            4.0|\n",
      "|      NZ|            5.0|\n",
      "|      SE|            6.0|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_faaa50b96c65\n",
       "DFindexed: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer()\n",
    "          .setInputCol(\"country2\")\n",
    "          .setOutputCol(\"country_indexed\")\n",
    "\n",
    "val DFindexed = indexer.fit(rescaledData).transform(rescaledData)\n",
    "DFindexed.select(\"country2\",\"country_indexed\").distinct().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 6 : convertir **currency2** en quantités numériques dans une colonne **currency_indexed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|currency2|currency_indexed|\n",
      "+---------+----------------+\n",
      "|      GBP|             1.0|\n",
      "|      NZD|             5.0|\n",
      "|      DKK|             7.0|\n",
      "|      AUD|             3.0|\n",
      "|      CAD|             2.0|\n",
      "|      EUR|             4.0|\n",
      "|      USD|             0.0|\n",
      "|      SEK|             6.0|\n",
      "|      NOK|             8.0|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexerCur: org.apache.spark.ml.feature.StringIndexer = strIdx_a7c28c7ccada\n",
       "DFindexedCur: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexerCur = new StringIndexer()\n",
    "          .setInputCol(\"currency2\")\n",
    "          .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "val DFindexedCur = indexerCur.fit(DFindexed).transform(DFindexed)\n",
    "DFindexedCur.select(\"currency2\",\"currency_indexed\").distinct().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stages 7 et 8: One-Hot encoder ces deux catégories avec un \"one-hot encoder\" en créant les colonnes **country_onehot** et **currency_onehot.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|country_onehot|currency_onehot|\n",
      "+--------------+---------------+\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[4],[1.0])|  (8,[4],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[0],[1.0])|  (8,[0],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "|(10,[1],[1.0])|  (8,[1],[1.0])|\n",
      "+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oneHotEncoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_167a84521c5d\n",
       "DFOne: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 20 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "          .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "          .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "val DFOne = oneHotEncoder.fit(DFindexedCur).transform(DFindexedCur)\n",
    "DFOne.select(\"country_onehot\",\"currency_onehot\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- deadline2: string (nullable = true)\n",
      " |-- created_at2: string (nullable = true)\n",
      " |-- launched_at2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- country_indexed: double (nullable = false)\n",
      " |-- currency_indexed: double (nullable = false)\n",
      " |-- country_onehot: vector (nullable = true)\n",
      " |-- currency_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFOne.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mettre les données sous une forme utilisable par Spark.ML :\n",
    "\n",
    "La plupart des algorithmes de machine learning dans Spark requièrent que les colonnes utilisées en input du modèle (les features du modèle) soient regroupées dans une seule colonne qui contient des vecteurs. On veut donc passer de \n",
    "\n",
    "| *Feature A* | *Feature B* | *Feature C* | *Label*  |\n",
    "| :---------: | :---------: | :---------: | :------: |\n",
    "| 0.5         | 1           | 3.5         | 0        |\n",
    "| 0.6         | 1           | 1.2         | 1        \n",
    "\n",
    "à\n",
    "\n",
    "|*Features*     |*Label*|\n",
    "|:-------------:|:-----:|\n",
    "| (0.5, 1, 3.5) | 0     |\n",
    "| (0.6, 1, 1.2) | 1     |\n",
    "\n",
    "\n",
    "- Stage 9 : assembler tous les features en un unique vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       join_features|\n",
      "+--------------------+\n",
      "|(97877,[33,43,91,...|\n",
      "|(97877,[700,874,8...|\n",
      "|(97877,[0,7,87,30...|\n",
      "|(97877,[0,60,86,1...|\n",
      "|(97877,[3,10,13,6...|\n",
      "|(97877,[88,114,15...|\n",
      "|(97877,[2,19,44,6...|\n",
      "|(97877,[11,19,31,...|\n",
      "|(97877,[9,118,288...|\n",
      "|(97877,[49,79,218...|\n",
      "|(97877,[9,10,23,1...|\n",
      "|(97877,[7,33,184,...|\n",
      "|(97877,[4,14,105,...|\n",
      "|(97877,[49,103,11...|\n",
      "|(97877,[32,42,110...|\n",
      "|(97877,[48,90,150...|\n",
      "|(97877,[110,571,8...|\n",
      "|(97877,[1,18,42,4...|\n",
      "|(97877,[3,52,53,6...|\n",
      "|(97877,[33,89,260...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_3eb77e3f98ae\n",
       "DFGroupFeat: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"features\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "  .setOutputCol(\"join_features\")\n",
    "\n",
    "val DFGroupFeat = assembler.transform(DFOne)\n",
    "DFGroupFeat.select(\"join_features\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 10 : créer/instancier le modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_5d9197fccd6f\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "    .setElasticNetParam(0.0)\n",
    "    .setFitIntercept(true)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"final_status\")\n",
    "    .setStandardization(true)\n",
    "    .setPredictionCol(\"predictions\")\n",
    "    .setRawPredictionCol(\"raw_predictions\")\n",
    "    .setThresholds(Array(0.7, 0.3))\n",
    "    .setTol(1.0e-6)\n",
    "    .setMaxIter(50) //comparer les deux valeurs et commenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du Pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myPipeline: org.apache.spark.ml.Pipeline = pipeline_1c315e5f4e56\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myPipeline = new Pipeline().setStages(Array(tokenizer, remover, \n",
    "                                                countVectorizer, idf, \n",
    "                                                indexer, indexerCur, \n",
    "                                                oneHotEncoder, assembler, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement, test, et sauvegarde du modèle :\n",
    "- Split des données en training et test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = myDataFrame.randomSplit(Array(0.9, 0.1), 98765L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1: org.apache.spark.ml.PipelineModel = pipeline_1c315e5f4e56\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model1 = myPipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1892|\n",
      "|           0|        1.0| 2310|\n",
      "|           1|        1.0| 1589|\n",
      "|           0|        0.0| 4963|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfWithSimplePredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_7c160f7e701d\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithSimplePredictions = model1.transform(test)\n",
    "dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                    .setLabelCol(\"final_status\")\n",
    "                    .setPredictionCol(\"predictions\")\n",
    "                    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle simple\n",
      "f1score du modele simple sur les donnees = %.3f\n",
      "0.6145469784062658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f1score: Double = 0.6145469784062658\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Modèle simple\")\n",
    "val f1score = evaluator.evaluate(dfWithSimplePredictions)\n",
    "println(\"f1score du modele simple sur les donnees = %.3f\\n\" + f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réglage des hyper-paramètres (a.k.a. tuning) du modèle :\n",
    "\n",
    "La façon de procéder présentée plus haut permet rapidement d'entraîner un modèle et d'avoir une mesure de sa performance. Mais que se passe-t-il si \n",
    "\n",
    "    - on souhaite utiliser 300 itérations au maximum plutôt que 50 (i.e. la ligne .setMaxIter(50) ) ? \n",
    "    \n",
    "**Réponse :**\n",
    "\n",
    "Avec un nombre maximum d'itérations réglé à 300, on constate que les **temps de calculs** deviennent beaucoup plus longs et même parfois le calcul n'est pas finalisé en raison de la consommation importante de **mémoire vive**.\n",
    "\n",
    "    - on souhaite modifier le paramètre de régularisation du modèle ? \n",
    "\n",
    "**Réponse :**\n",
    "\n",
    "Si on souhaite modifier le paramètre de régularisation du modèle il faut relancer tout le pipeline, ce qui n'est pas pratique et on arrive difficilement à trouver le paramètre optimum en procédant ainsi. On va voir que la méthode de grid search permet de trouver de manière pratique les hyperparamètres optimums pour le modèle.\n",
    "\n",
    "\n",
    "\n",
    "    - on souhaite modifier le paramètre minDF de la classe CountVectorizer (qui permet de ne prendre que les mots apparaissant dans au moins minDF documents) ?\n",
    "\n",
    "**Réponse :**\n",
    "\n",
    "Le fait de négliger les mots qui n'apparaissent que très peu de fois dans le texte devrait permettre de robustifier le modèle, de le rendre plus général. On accélère aussi surtout le processing dans la mesure on ne ne considère pas les mots que n'apparaissent pratiquement pas, on limite le nombre de mots sur lequel on fait le processing, donc **on réduit le temps et la mémoire nécessaires au calculs.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faudrait à chaque fois modifier le(s) paramètre(s) à la main, ré-entraîner le modèle, re-calculer la performance du modèle obtenu sur l'ensemble de test, puis finalement choisir le meilleur modèle (i.e. celui avec la meilleure performance sur les données de test) parmi tous ces modèles entraînés. C'est ce qu'on appelle le réglage des hyper-paramètres ou encore tuning du modèle. Et c'est fastidieux.\n",
    "\n",
    "\n",
    "La plupart des algorithmes de machine learning possèdent des hyper-paramètres, par exemple le nombre de couches et de neurones dans un réseau de neurones, le nombre d’arbres et leur profondeur maximale dans les random forests, etc. Qui plus est, comme mentionné précédemment avec le paramètre minDF de la classe CountVectorizer, on peut également se retrouver avec des hyper-paramètres au niveau des stages de préprocessing. L'objectif est donc de trouver la meilleure combinaison possible de tous ces hyper-paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Grid search**\n",
    "\n",
    "Une des techniques pour régler automatiquement les hyper-paramètres est la grid\n",
    "search qui consiste à : \n",
    "\n",
    "   - créer une grille de valeurs à tester pour les hyper-paramètres \n",
    "    \n",
    "   - en chaque point de la grille \n",
    "    \n",
    "        - séparer le training set en un ensemble de training (70%) et un ensemble de validation (30%)\n",
    "    \n",
    "        - entraîner un modèle sur le training set\n",
    "    \n",
    "        - calculer l’erreur du modèle sur le validation set\n",
    "\n",
    "   - sélectionner le point de la grille (<=> garder les valeurs d’hyper-paramètres de ce point) où l’erreur de validation est la plus faible i.e. là où le modèle a le mieux appris\n",
    "    \n",
    "Pour la régularisation de notre régression logistique on veut tester les valeurs de 10e-8 à 10e-2 par pas de 2.0 en échelle logarithmique (on veut tester les valeurs 10e-8, 10e-6, 10e-4 et 10e-2). Pour le paramètre minDF de CountVectorizer on veut tester les valeurs de 55 à 95 par pas de 20. En chaque point de la grille on veut utiliser 70% des données pour l’entraînement et 30% pour la validation. On veut utiliser le **f1-score** pour comparer les différents modèles en chaque point de la grille. \n",
    "\n",
    "Préparer la grid-search pour satisfaire les conditions explicitées ci-dessus puis lancer la grid-search sur le dataset \"training\" préparé précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_59c226a0eac4-minDF: 55.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 55.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 55.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 55.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 0.1\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 75.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 75.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 75.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 75.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 0.1\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 95.0,\n",
       "\tlogreg_5d9197fccd6f-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_59c226a0eac4-minDF: 95.0,\n",
       "\tlogreg_5d9197f..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Réglage des hyper-paramètres du modèle\n",
    "// par Grid search\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "      .addGrid(countVectorizer.minDF, Array[Double](55, 75, 95))\n",
    "      .build()\n",
    "\n",
    "val cross_valid = new TrainValidationSplit()\n",
    "      .setEstimator(myPipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setTrainRatio(0.7) // 70% training, 30% validation\n",
    "\n",
    "val model2 = cross_valid.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test du modèle\n",
    "\n",
    "On a vu que pour évaluer de façon non biaisée la pertinence du modèle obtenu, il fallait le tester sur des données qu'il n'avait jamais vues pendant son entraînement. Ça vaut également pour les données utilisées pour sélectionner le meilleur modèle de la grid search (training et validation)! C’est pour cela que nous avons construit le dataset de test que nous avons laissé de côté jusque là. \n",
    "\n",
    "Appliquer le meilleur modèle trouvé avec la grid-search aux données de test. Mettre les résultats dans le DataFrame dfWithPredictions . Afficher le f1-score du modèle sur les données de test. \n",
    "\n",
    "Afficher \n",
    "\n",
    "dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "Sauvegarder le modèle entraîné pour pouvoir le réutiliser plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1093|\n",
      "|           0|        1.0| 2941|\n",
      "|           1|        1.0| 2388|\n",
      "|           0|        0.0| 4332|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfWithPredictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithPredictions: DataFrame = model2.transform(test)\n",
    "dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle paramétrique (grid search)\n",
      "f1score du modele paramétrique (avec grid search) sur les données = 0.637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f1score2: Double = 0.6369300415318682\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Modèle paramétrique (grid search)\")\n",
    "val f1score2 = evaluator.evaluate(dfWithPredictions)\n",
    "println(\"f1score du modele paramétrique (avec grid search) sur les données = %.3f\".format(f1score2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplément\n",
    "Pour plus d’information sur la façon dont est parallélisée la méthode de Newton (pour trouver le maximum de la fonction de coût définissant la régression logistique, qui est le log de la vraisemblance) :\n",
    "\n",
    "- [http://www.slideshare.net/dbtsai/2014-0620-mlor-36132297](http://www.slideshare.net/dbtsai/2014-0620-mlor-36132297)\n",
    "\n",
    "- [http://www.research.rutgers.edu/~lihong/pub/Zinkevich11Parallelized.pdf](http://www.research.rutgers.edu/~lihong/pub/Zinkevich11Parallelized.pdf)\n",
    "     \n",
    "- [https://arxiv.org/pdf/1605.06049v1.pdf](https://arxiv.org/pdf/1605.06049v1.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
